{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "import librosa\n",
    "import librosa.display as DSP\n",
    "import librosa as lr\n",
    "from tempfile import mktemp\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "import pylab\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.io.wavfile import read\n",
    "import python_speech_features as mfcc\n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.metrics import f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(array):\n",
    "   \n",
    "    rows, cols = array.shape\n",
    "    # print(rows)\n",
    "    # print(cols)\n",
    "    deltas = np.zeros((rows,20))\n",
    "    N = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= N:\n",
    "            if i-j < 0:\n",
    "                first =0\n",
    "            else:\n",
    "                first = i-j\n",
    "            if i+j > rows-1:\n",
    "                second = rows-1\n",
    "            else:\n",
    "                second = i+j \n",
    "            index.append((second,first))\n",
    "            j+=1\n",
    "        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio,rate):\n",
    "       \n",
    "    mfcc_feature = mfcc.mfcc(audio, rate, 0.025, 0.01, 20, nfft = 2205, appendEnergy = True)    \n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "#     print(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature, delta)) \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "lstFolders = ['Doha', 'shuaib', 'rabea', 'Rahma']\n",
    "for lst in lstFolders:\n",
    "    listFiles = glob.glob(r\"Training Data\\{}\\*\".format(lst))\n",
    "    features = np.asarray(())\n",
    "    finishCounter = len(listFiles)\n",
    "    counter = 1\n",
    "    for file in listFiles:\n",
    "        try:\n",
    "            audio, sr = librosa.load(file)\n",
    "            vector = extract_features(audio, sr)\n",
    "            \n",
    "            if features.size == 0:\n",
    "                features = vector\n",
    "            else:\n",
    "                features = np.vstack((features, vector))\n",
    "                \n",
    "            X_test.append(vector)\n",
    "            y_test.append(lstFolders.index(lst))\n",
    "                \n",
    "            counter += 1\n",
    "            if(counter == finishCounter):\n",
    "                gmm = GaussianMixture(n_components = 6, max_iter = 200, covariance_type='diag', n_init = 3)\n",
    "                gmm.fit(features)\n",
    "                \n",
    "                # dumping the trained gaussian model\n",
    "                picklefile = lst + \".joblib\"\n",
    "                joblib.dump(gmm, picklefile) \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_files = [ i + '.joblib' for i in ['Doha', 'shuaib', 'rabea', 'Rahma']]\n",
    "\n",
    "models    = [joblib.load(fname) for fname in gmm_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y = []\n",
    "y = []\n",
    "for i in X_test:\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for j in range(len(models)):\n",
    "        gmm = models[j] \n",
    "        scores = np.array(gmm.score(i))\n",
    "        log_likelihood[j] = scores.sum()\n",
    "    y.append(log_likelihood)\n",
    "\n",
    "    winner = np.argmax(log_likelihood)\n",
    "    final_y.append(winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(final_y, y_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdetected as - GaussianMixture(covariance_type='diag', max_iter=200, n_components=6, n_init=3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "file = r'C:\\Users\\DELL\\Desktop\\simple-recorder3\\Training Data word\\Open_book\\rahma_open_book6 (4).wav'\n",
    "audio, sr = librosa.load(file)\n",
    "vector = extract_features(audio, sr)\n",
    "og_likelihood = np.zeros(len(models)) \n",
    "for i in range(len(models)):\n",
    "    gmm    = models[i] \n",
    "    scores = np.array(gmm.score(vector))\n",
    "    log_likelihood[i] = scores.sum()\n",
    "y.append(log_likelihood)\n",
    "\n",
    "winner = np.argmax(log_likelihood)\n",
    "print(\"\\tdetected as -\",models[winner])\n",
    "print(winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         ... 0.00222425 0.0028127  0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Desktop\\RecordeR\\train.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/RecordeR/train.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/RecordeR/train.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/RecordeR/train.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_principal \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(audio) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/RecordeR/train.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_principal \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_principal) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Desktop/RecordeR/train.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_principal\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mP1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mP2\u001b[39m\u001b[39m'\u001b[39m] \n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:433\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    412\u001b[0m     \u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \n\u001b[0;32m    414\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    434\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    437\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[1;32m--> 456\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    457\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.         0.         0.         ... 0.00222425 0.0028127  0.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) \n",
    "X_principal = pca.fit_transform(audio) \n",
    "X_principal = pd.DataFrame(X_principal) \n",
    "X_principal.columns = ['P1', 'P2'] \n",
    "import matplotlib.pyplot as plt \n",
    "plt.scatter(X_principal[1], X_principal[2],\n",
    "           c = GaussianMixture(n_components = 2).fit_predict(X_principal), cmap =plt.cm.winter, alpha = 0.6) \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
